{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Implementing a Perciever in JAX for fun âœ¨\"\n",
    "author:\n",
    "  - name: \"Tugdual Kerjan\"\n",
    "    url: https://tugdual.fr\n",
    "    email: tkerjan@outlook.com\n",
    "date: \"November 15, 2024\"\n",
    "number-sections: true\n",
    "reference-location: margin\n",
    "toc: true\n",
    "format: \n",
    "  html:\n",
    "    standalone: true\n",
    "    embed-resources: true\n",
    "    self-contained-math: true\n",
    "    code-fold: false\n",
    "    code-tools: true\n",
    "execute:\n",
    "  output:\n",
    "    false\n",
    "bibliography: assets/bib.bibtex\n",
    "theme: united\n",
    "github: \"https://github.com/TugdualKerjan/ResNet-for-JAX\"\n",
    "lightbox: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the full project, visit the [GitHub repository](https://github.com/TugdualKerjan/Adventures-in-Equinox).\n",
    "\n",
    "# Context ðŸ‘€\n",
    "\n",
    "I'm trying to rewrite XTTS in JAX to understand how it works. \n",
    "\n",
    "We are going to implement the Perceiver Resampler, a model introduced by [@alayrac2022flamingovisuallanguagemodel] to address the inefficiency of attention mechanisms in processing high-dimensional or multimodal inputs. Unlike traditional transformers, here input features are projected into a fixed-size latent array using a resampling module, which efficiently extracts and compresses information of varying input size. We thus avoid having to constrain ourselves to a fixed input. This work builds on the Perciever [@jaegle2021perceivergeneralperceptioniterative], a paper proposing fixed latent arrays as input to attention modules, improving the costly quadratic compute costs of attention modules. \n",
    "\n",
    "::: {.column-margin}\n",
    "\n",
    "![A high level overview of Flamingo, a model based on this concept from [@alayrac2022flamingovisuallanguagemodel]](assets/arch.png)\n",
    "\n",
    ":::\n",
    "\n",
    "__Attention__\n",
    "\n",
    "The most important part of this model is the concept of attention: It takes the input and learns to find which parts are relevant to one another. Seeing two eyes and a mouth in your input can allow you to infer that there is a face for example.\n",
    "\n",
    "__Resampling__\n",
    "\n",
    "My favorite part - to be agnostic to input length, the input is flattened and the equivalent of positional embeddings are added.\n",
    "\n",
    "![Resampling of the input [@alayrac2022flamingovisuallanguagemodel]](assets/resampler.png)\n",
    "\n",
    "# Goal ðŸŽ¯\n",
    "\n",
    "Train a Perciever Resampler on ??? !\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing our favorite libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import equinox.nn as nn\n",
    "from typing import Optional\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GEGLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "class GEGLU(eqx.Module):\n",
    "    def __call__(self, x):\n",
    "        x, gate = jnp.split(x, 2, axis=-1)\n",
    "        return jax.nn.gelu(gate, approximate=False) * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test that this works correctly with some quick code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.tts.layers.xtts.perceiver_encoder import GEGLU as theirGEGLU\n",
    "\n",
    "our_x = jax.random.normal(jax.random.PRNGKey(1), shape=(30,))\n",
    "their_x = torch.from_numpy(np.array(our_x))\n",
    "\n",
    "ours = GEGLU()\n",
    "theirs = theirGEGLU()\n",
    "\n",
    "our_y = GEGLU()(our_x)\n",
    "their_y = theirGEGLU()(their_x)\n",
    "\n",
    "torch.testing.assert_close(their_y, torch.from_numpy(np.array(our_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CausalConv1d\n",
    "\n",
    "Since our model has to predict the next token, having convolution layers that map surrounding tokens would allow the model to cheat by incoporating future token information during the convolutions. Because of this, we create a custom layer that pads away the future tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "class CausalConv1d(eqx.nn.Conv1d):\n",
    "    causal_padding: int\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        (kernel_size,) = self.kernel_size\n",
    "        (dilation,) = self.dilation\n",
    "        (stride,) = self.stride\n",
    "\n",
    "        assert stride == 1\n",
    "        self.causal_padding = dilation * (kernel_size - 1)\n",
    "\n",
    "    def __call__(self, x: jax.Array, *, key: Optional[jax.Array] = None) -> jax.Array:\n",
    "        causal_padded_x = jax.numpy.pad(\n",
    "            x, ((0, 0), (self.causal_padding, 0)), mode=\"constant\", constant_values=0.0\n",
    "        )\n",
    "        # print(causal_padded_x.shape)\n",
    "        return super().__call__(causal_padded_x, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal_padding\n"
     ]
    }
   ],
   "source": [
    "from TTS.tts.layers.xtts.perceiver_encoder import CausalConv1d as theirGEGLU\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "our_x = jax.random.normal(jax.random.PRNGKey(1), shape=(2, 10))\n",
    "their_x = torch.from_numpy(np.array(our_x))\n",
    "\n",
    "ours = CausalConv1d(2, 2, 3, key=jax.random.PRNGKey(2))\n",
    "theirs = theirGEGLU(2, 2, 3)\n",
    "\n",
    "their_params = {key: value for key, value in theirs.named_parameters()}\n",
    "\n",
    "\n",
    "def update_weights(path, x):\n",
    "    path = \".\".join([str(p).strip(\"[].\") for p in path])\n",
    "    if path in their_params:\n",
    "        if \"bias\" in path:\n",
    "            return jnp.expand_dims(their_params[path].detach().numpy(), -1)\n",
    "        return jnp.array(their_params[path].detach().numpy())\n",
    "    print(path)\n",
    "    return x\n",
    "\n",
    "\n",
    "ours = jax.tree_util.tree_map_with_path(update_weights, ours)\n",
    "\n",
    "our_y = ours(our_x)\n",
    "their_y = theirs(their_x)\n",
    "\n",
    "torch.testing.assert_close(their_y, torch.from_numpy(np.array(our_y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "class Attend(eqx.Module):\n",
    "    dropout: float\n",
    "    causal: bool\n",
    "    attn_dropout: nn.Dropout\n",
    "\n",
    "    def __init__(self, dropout=0.0, causal=False, use_flash=False):\n",
    "        self.dropout = dropout\n",
    "        self.attn_dropout = eqx.nn.Dropout(dropout, inference=True)\n",
    "\n",
    "        self.causal = causal\n",
    "\n",
    "    def get_mask(self, n):\n",
    "        return jnp.triu(jnp.ones((n, n), dtype=bool), k=1)\n",
    "\n",
    "    def __call__(self, q, k, v, mask=None):\n",
    "        n = q.shape[-2]\n",
    "        scale = q.shape[-1] ** -0.5\n",
    "        kq = jnp.matmul(q, jnp.transpose(k, (0, 2, 1))) * scale\n",
    "        # Key mask\n",
    "        if mask is not None:\n",
    "            mask = jnp.expand_dims(mask, 0)\n",
    "            kq = jnp.where(mask, kq, jnp.zeros_like(mask))\n",
    "\n",
    "        if self.causal:\n",
    "            kq = jax.numpy.where(self.get_mask(n), kq, -jnp.finfo(kq.dtype).max)\n",
    "\n",
    "        attn = jax.nn.softmax(kq, axis=-1)\n",
    "        attn = self.attn_dropout(attn)\n",
    "\n",
    "        out = jnp.matmul(attn, v)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropout\n",
      "causal\n",
      "attn_dropout.p\n",
      "attn_dropout.inference\n",
      "(2, 24, 10)\n"
     ]
    }
   ],
   "source": [
    "from TTS.tts.layers.xtts.perceiver_encoder import Attend as theirAttend\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "ours = Attend(0.1, causal=False, use_flash=False)\n",
    "theirs = theirAttend(0.1, causal=False, use_flash=False)\n",
    "\n",
    "x = jax.random.normal(jax.random.PRNGKey(1), shape=(6, 24, 10))\n",
    "our_x, our_k, our_v = jax.numpy.split(x, 3, axis=0)\n",
    "their_q = torch.from_numpy(np.array(our_x))\n",
    "their_k = torch.from_numpy(np.array(our_k))\n",
    "their_v = torch.from_numpy(np.array(our_v))\n",
    "\n",
    "\n",
    "their_params = {key: value for key, value in theirs.named_parameters()}\n",
    "\n",
    "\n",
    "def update_weights(path, x):\n",
    "    path = \".\".join([str(p).strip(\"[].\") for p in path])\n",
    "    if path in their_params:\n",
    "        if \"bias\" in path:\n",
    "            return jnp.expand_dims(their_params[path].detach().numpy(), -1)\n",
    "        return jnp.array(their_params[path].detach().numpy())\n",
    "    print(path)\n",
    "    return x\n",
    "\n",
    "\n",
    "ours = jax.tree_util.tree_map_with_path(update_weights, ours)\n",
    "our_y = ours(our_x, our_k, our_v)\n",
    "\n",
    "their_q = their_q.unsqueeze(0)\n",
    "their_k = their_k.unsqueeze(0)\n",
    "their_v = their_v.unsqueeze(0)\n",
    "\n",
    "their_y = theirs(their_q, their_k, their_v)\n",
    "\n",
    "their_y = their_y.squeeze(0)\n",
    "\n",
    "torch.testing.assert_close(their_y, torch.from_numpy(np.array(our_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "class Attention(eqx.Module):\n",
    "\n",
    "    cross_attn_include_queries: bool\n",
    "    scale: float\n",
    "    heads: int\n",
    "\n",
    "    attend: Attend\n",
    "    to_q: nn.Linear\n",
    "    to_kv: nn.Linear\n",
    "    to_out: nn.Linear\n",
    "\n",
    "    dim_inner: int\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        dim_context=None,\n",
    "        causal=False,\n",
    "        dim_head=64,\n",
    "        heads=8,\n",
    "        dropout=0.0,\n",
    "        use_flash=False,\n",
    "        cross_attn_include_queries=False,\n",
    "        key=None,\n",
    "    ):\n",
    "        key1, key2, key3 = jax.random.split(key, 3)\n",
    "\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        self.cross_attn_include_queries = cross_attn_include_queries\n",
    "        self.dim_inner = dim_head * heads\n",
    "\n",
    "        self.attend = Attend(dropout, causal)\n",
    "\n",
    "        self.to_q = nn.Linear(dim, self.dim_inner, use_bias=False, key=key1)\n",
    "        self.to_kv = nn.Linear(dim, self.dim_inner * 2, use_bias=False, key=key2)\n",
    "        self.to_out = nn.Linear(self.dim_inner, dim, use_bias=False, key=key3)\n",
    "\n",
    "    # @partial(jax.jit, static_argnums=2)\n",
    "    def __call__(self, x, context, mask=None):\n",
    "\n",
    "        # Should the kv, cross attention, include the query values ?\n",
    "        context = jnp.concat([x, context], axis=-2)\n",
    "        q, k, v = (\n",
    "            jax.vmap(jax.vmap(self.to_q))(x),\n",
    "            *jnp.split(jax.vmap(jax.vmap(self.to_kv))(context), 2, axis=-1),\n",
    "        )\n",
    "        # q = jnp.reshape(q, shape=(q.shape[0], self.heads, q.shape[-2], -1))\n",
    "\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b n (h d) -> b h n d\", h=self.heads), (q, k, v)\n",
    "        )\n",
    "\n",
    "        # k = jnp.reshape(k, shape=(k.shape[0], self.heads, k.shape[-2], -1))\n",
    "        # v = jnp.reshape(v, shape=(v.shape[0], self.heads, v.shape[-2], -1))\n",
    "        out = jax.vmap(self.attend)(q, k, v, mask)\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "\n",
    "        return jax.vmap(jax.vmap(self.to_out))(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross_attn_include_queries\n",
      "scale\n",
      "heads\n",
      "attend.dropout\n",
      "attend.causal\n",
      "attend.attn_dropout.p\n",
      "attend.attn_dropout.inference\n",
      "dim_inner\n",
      "2.78 ms Â± 62.3 Î¼s per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n",
      "214 Î¼s Â± 16.3 Î¼s per loop (mean Â± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from TTS.tts.layers.xtts.perceiver_encoder import Attention as theirAttention\n",
    "import torch\n",
    "import equinox.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "ours = Attention(\n",
    "    6,\n",
    "    causal=False,\n",
    "    dim_head=4,\n",
    "    heads=2,\n",
    "    dropout=0,\n",
    "    cross_attn_include_queries=True,\n",
    "    key=jax.random.PRNGKey(1),\n",
    ")\n",
    "theirs = theirAttention(\n",
    "    6, causal=False, dim_head=4, heads=2, dropout=0, cross_attn_include_queries=True\n",
    ")\n",
    "\n",
    "x = jax.random.normal(jax.random.PRNGKey(1), shape=(2, 10, 6))\n",
    "\n",
    "our_latents, our_x = jax.numpy.split(x, 2, axis=1)\n",
    "\n",
    "their_latents = torch.from_numpy(np.array(our_latents))\n",
    "their_x = torch.from_numpy(np.array(our_x))\n",
    "their_q = their_q.unsqueeze(0)\n",
    "their_k = their_k.unsqueeze(0)\n",
    "\n",
    "their_params = {key: value for key, value in theirs.named_parameters()}\n",
    "\n",
    "\n",
    "def update_weights(path, x):\n",
    "    path = \".\".join([str(p).strip(\"[].\") for p in path])\n",
    "    if path in their_params:\n",
    "        if \"bias\" in path:\n",
    "            return jnp.expand_dims(their_params[path].detach().numpy(), -1)\n",
    "        return jnp.array(their_params[path].detach().numpy())\n",
    "    print(path)\n",
    "    return x\n",
    "\n",
    "\n",
    "ours = jax.tree_util.tree_map_with_path(update_weights, ours)\n",
    "\n",
    "%timeit our_y = ours(our_latents, our_x, mask=None)\n",
    "\n",
    "%timeit their_y = theirs(their_latents, their_x)\n",
    "\n",
    "# their_y = their_y.squeeze(0)\n",
    "\n",
    "torch.testing.assert_close(their_y, torch.from_numpy(np.array(our_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "class FeedForward(eqx.Module):\n",
    "    causal_conv: bool\n",
    "    ff1: nn.Linear\n",
    "    ff2: nn.Linear\n",
    "    act: GEGLU\n",
    "    conv: CausalConv1d\n",
    "\n",
    "    def __init__(self, dim, mult=4, causal_conv=False, key=None):\n",
    "        key1, key2, key3 = jax.random.split(key, 3)\n",
    "\n",
    "        self.causal_conv = causal_conv\n",
    "        dim_inner = int(dim * mult * 2 / 3)\n",
    "        self.conv = CausalConv1d(dim_inner, dim_inner, 3, key=key3)\n",
    "        self.act = GEGLU()\n",
    "        self.ff1 = nn.Linear(dim, dim_inner * 2, key=key1)\n",
    "        self.ff2 = nn.Linear(dim_inner, dim, key=key2)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        y = jax.vmap(self.ff1)(x)\n",
    "        y = self.act(y)\n",
    "        if self.causal_conv:\n",
    "            y = jnp.permute_dims(y, (1, 0))\n",
    "            y = self.conv(y)\n",
    "            y = jnp.permute_dims(y, (1, 0))\n",
    "        y = jax.vmap(self.ff2)(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "causal_conv\n",
      "ff1.weight\n",
      "Param ff1.weight has shape (160, 30)\n",
      "ff1.bias\n",
      "Param ff1.bias has shape (160,)\n",
      "ff2.weight\n",
      "Param ff2.weight has shape (30, 80)\n",
      "ff2.bias\n",
      "Param ff2.bias has shape (30,)\n",
      "conv.weight\n",
      "Param conv.weight has shape (80, 80, 3)\n",
      "conv.bias\n",
      "Param conv.bias has shape (80, 1)\n",
      "conv.causal_padding\n",
      "causal_conv\n",
      "ff1.weight\n",
      "Param ff1.weight has shape (160, 30)\n",
      "ff1.bias\n",
      "Param ff1.bias has shape (160,)\n",
      "ff2.weight\n",
      "Param ff2.weight has shape (30, 80)\n",
      "ff2.bias\n",
      "Param ff2.bias has shape (30,)\n",
      "conv.weight\n",
      "Param conv.weight has shape (80, 80, 3)\n",
      "conv.bias\n",
      "Param conv.bias has shape (80, 1)\n",
      "conv.causal_padding\n",
      "Incoming (160, 30)\n",
      "Incoming (160, 160)\n"
     ]
    }
   ],
   "source": [
    "from TTS.tts.layers.xtts.perceiver_encoder import FeedForward as theirFF\n",
    "import torch\n",
    "import equinox.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "ours = FeedForward(30, 4, False, key=jax.random.PRNGKey(1))\n",
    "theirs = theirFF(30, 4, False)\n",
    "\n",
    "our_x = jax.random.normal(jax.random.PRNGKey(1), shape=(2, 160, 30))\n",
    "their_x = torch.from_numpy(np.array(our_x))\n",
    "\n",
    "their_params = {key: value for key, value in theirs.named_parameters()}\n",
    "\n",
    "mapping = {\n",
    "    \"ff1.weight\": \"0.weight\",\n",
    "    \"ff1.bias\": \"0.bias\",\n",
    "    \"ff2.weight\": \"2.weight\",\n",
    "    \"ff2.bias\": \"2.bias\",\n",
    "}\n",
    "\n",
    "\n",
    "def update_weights(path, x):\n",
    "    path = \".\".join([str(p).strip(\"[].\") for p in path])\n",
    "    if path in their_params:\n",
    "        if \"bias\" in path and \"conv\" not in path:\n",
    "            return jnp.expand_dims(their_params[path].detach().numpy(), -1)\n",
    "        return jnp.array(their_params[path].detach().numpy())\n",
    "    if path in mapping.keys():\n",
    "        if \"bias\" in path:\n",
    "            return jnp.array(their_params[mapping[path]].detach().numpy())\n",
    "        return jnp.array(their_params[mapping[path]].detach().numpy())\n",
    "    return x\n",
    "\n",
    "\n",
    "def print_shapes(path, x):\n",
    "    path = \".\".join([str(p).strip(\"[].\") for p in path])\n",
    "    print(path)\n",
    "\n",
    "    if \"weight\" in path or \"bias\" in path:\n",
    "        print(f\"Param {path} has shape {x.shape}\")\n",
    "\n",
    "\n",
    "jax.tree_util.tree_map_with_path(print_shapes, ours)\n",
    "ours = jax.tree_util.tree_map_with_path(update_weights, ours)\n",
    "jax.tree_util.tree_map_with_path(print_shapes, ours)\n",
    "\n",
    "\n",
    "our_y = jax.vmap(ours)(our_x)\n",
    "their_y = theirs(their_x)\n",
    "\n",
    "# their_y = their_y.squeeze(0)\n",
    "\n",
    "torch.testing.assert_close(their_y, torch.from_numpy(np.array(our_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "export"
    ]
   },
   "outputs": [],
   "source": [
    "from einops import repeat\n",
    "\n",
    "\n",
    "class PerceiverResampler(eqx.Module):\n",
    "\n",
    "    proj_context: jax.Array\n",
    "    latents: jax.Array\n",
    "    layers: list\n",
    "    norm: RMSNorm\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        depth=2,\n",
    "        dim_context=None,\n",
    "        num_latents=32,\n",
    "        dim_head=64,\n",
    "        heads=8,\n",
    "        ff_mult=4,\n",
    "        use_flash_attn=False,\n",
    "        key=None,\n",
    "    ):\n",
    "\n",
    "        key1, key2, key3 = jax.random.split(key, 3)\n",
    "        if dim_context is None:\n",
    "            dim_context = dim\n",
    "\n",
    "        self.proj_context = (\n",
    "            nn.Linear(dim_context, dim, key=key1)\n",
    "            if dim != dim_context\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "        self.latents = jax.random.normal(key3, (num_latents, dim))\n",
    "\n",
    "        self.layers = [\n",
    "            (\n",
    "                Attention(\n",
    "                    dim=dim,\n",
    "                    dim_head=dim_head,\n",
    "                    heads=heads,\n",
    "                    use_flash=use_flash_attn,\n",
    "                    cross_attn_include_queries=True,\n",
    "                    key=y1,\n",
    "                ),\n",
    "                FeedForward(dim=dim, mult=ff_mult, key=y1),\n",
    "            )\n",
    "            for y1 in jax.random.split(key2, depth)\n",
    "        ]\n",
    "\n",
    "        self.norm = RMSNorm(dim)\n",
    "\n",
    "    def __call__(self, x, mask=None):\n",
    "        # print(f\"Shape of x: {x.shape}\")\n",
    "        y = jax.vmap(self.proj_context)(x)\n",
    "        # print(f\"Shape of y: {y.shape}\")\n",
    "        latents = repeat(self.latents, \"n d -> b n d\", b=x.shape[0])\n",
    "        # print(f\"Shape of latent: {self.latents.shape}\")\n",
    "        # latents = j\n",
    "\n",
    "        for attn, ff in self.layers:\n",
    "            print(latents[0, 0])\n",
    "\n",
    "            latents = attn(latents, y, mask) + latents\n",
    "            print(latents[0, 0])\n",
    "            latents = jax.vmap(ff)(latents) + latents\n",
    "        return jax.vmap(jax.vmap(self.norm))(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fsspec.implementations.local.LocalFileOpener object at 0x3c3161bd0>\n",
      "<fsspec.implementations.local.LocalFileOpener object at 0x3174f2230>\n",
      "<fsspec.implementations.local.LocalFileOpener object at 0x3174f3640>\n",
      "<_io.BufferedReader name='checkpoints/dvae.pth'>\n",
      ">> DVAE weights restored from: checkpoints/dvae.pth\n",
      "<fsspec.implementations.local.LocalFileOpener object at 0x3174f1f00>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import jax\n",
    "import numpy as np\n",
    "import jax\n",
    "import equinox as eqx\n",
    "\n",
    "# Exported from .ipynb using python3 export.py (Exports all cells with the export tag)\n",
    "from TTS.tts.layers.xtts.trainer.gpt_trainer import (\n",
    "    GPTArgs,\n",
    "    GPTTrainerConfig,\n",
    "    GPTTrainer,\n",
    "    XttsAudioConfig,\n",
    ")\n",
    "\n",
    "# init args and config\n",
    "model_args = GPTArgs(\n",
    "    max_conditioning_length=132300,  # 6 secs\n",
    "    min_conditioning_length=11025,  # 0.5 secs\n",
    "    debug_loading_failures=False,\n",
    "    max_wav_length=220000,  # ~11.6 seconds\n",
    "    max_text_length=200,\n",
    "    mel_norm_file=\"checkpoints/mel_stats.pth\",\n",
    "    dvae_checkpoint=\"checkpoints/dvae.pth\",\n",
    "    xtts_checkpoint=\"./checkpoints/model.pth\",  # checkpoint path of the model that you want to fine-tune\n",
    "    tokenizer_file=\"checkpoints/vocab.json\",\n",
    "    gpt_num_audio_tokens=1026,\n",
    "    gpt_start_audio_token=1024,\n",
    "    gpt_stop_audio_token=1025,\n",
    "    gpt_use_masking_gt_prompt_approach=True,\n",
    "    gpt_use_perceiver_resampler=True,\n",
    ")\n",
    "# define audio config\n",
    "audio_config = XttsAudioConfig(\n",
    "    sample_rate=22050, dvae_sample_rate=22050, output_sample_rate=24000\n",
    ")\n",
    "# training parameters config\n",
    "\n",
    "config = GPTTrainerConfig()\n",
    "\n",
    "config.load_json(\"checkpoints/config.json\")\n",
    "\n",
    "config.epochs = 3\n",
    "config.output_path = \"checkpoints\"\n",
    "config.model_args = model_args\n",
    "config.audio = audio_config\n",
    "config.batch_size = 2\n",
    "config.num_loader_workers = 8\n",
    "config.eval_split_max_size = 256\n",
    "config.print_step = 50\n",
    "config.plot_step = 100\n",
    "config.log_model_step = 100\n",
    "config.save_step = 100\n",
    "config.save_n_checkpoints = 1\n",
    "config.save_checkpoints = True\n",
    "config.print_eval = False\n",
    "config.optimizer = \"AdamW\"\n",
    "config.optimizer_params = {\n",
    "    \"betas\": [0.9, 0.96],\n",
    "    \"eps\": 1e-8,\n",
    "    \"weight_decay\": 0.99,\n",
    "}\n",
    "config.lr = 1e-4\n",
    "config.lr_scheduler = \"MultiStepLR\"\n",
    "config.lr_scheduler_params = {\n",
    "    \"milestones\": [50000 * 18, 150000 * 18, 300000 * 18],\n",
    "    \"gamma\": 0.5,\n",
    "    \"last_epoch\": -1,\n",
    "}\n",
    "config.test_sentences = []\n",
    "\n",
    "# init the model from config\n",
    "model = GPTTrainer.init_from_config(config).xtts.gpt.conditioning_perceiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['latents', 'layers.0.0.to_q.weight', 'layers.0.0.to_kv.weight', 'layers.0.0.to_out.weight', 'layers.0.1.0.weight', 'layers.0.1.0.bias', 'layers.0.1.2.weight', 'layers.0.1.2.bias', 'layers.1.0.to_q.weight', 'layers.1.0.to_kv.weight', 'layers.1.0.to_out.weight', 'layers.1.1.0.weight', 'layers.1.1.0.bias', 'layers.1.1.2.weight', 'layers.1.1.2.bias', 'norm.gamma'])\n",
      "torch.Size([32, 1024])\n",
      "torch.Size([32, 1024])\n",
      "torch.Size([512, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024, 512])\n",
      "torch.Size([512, 1024])\n",
      "torch.Size([1024, 1024])\n",
      "torch.Size([1024, 512])\n",
      "[ 0.01236053  0.01541404 -0.00212941 ... -0.00038619 -0.00196526\n",
      " -0.00434487]\n",
      "[ 0.06295125  0.04312283  0.05844267 ... -0.11432284 -0.08922151\n",
      " -0.08559718]\n",
      "[ 0.00548464 -0.04037053 -0.13012525 ...  0.21880269 -0.38889858\n",
      " -0.07342979]\n",
      "[-4.2671423   2.1818194   0.6935338  ...  8.672872    0.17315355\n",
      " -1.6407791 ]\n",
      "torch.Size([2, 32, 1024])\n",
      "tensor([ 0.0124,  0.0154, -0.0021,  ..., -0.0004, -0.0020, -0.0043],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "wtf tensor([ 0.0630,  0.0431,  0.0584,  ..., -0.1143, -0.0892, -0.0856],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([ 0.0055, -0.0404, -0.1301,  ...,  0.2188, -0.3889, -0.0734],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "wtf tensor([-4.2671,  2.1818,  0.6935,  ...,  8.6729,  0.1732, -1.6408],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "[[-0.6711054   0.87788886 -1.5728431  ...  0.5993201  -0.3079374\n",
      "  -0.7011469 ]\n",
      " [ 0.47668818 -1.5089682   1.1775821  ...  0.23286052  0.5647733\n",
      "   1.0547528 ]\n",
      " [ 0.00828246  2.0027382  -1.4793181  ... -1.1489285  -0.37196496\n",
      "  -0.5069007 ]\n",
      " ...\n",
      " [-0.5160664  -0.49394092 -0.5420418  ...  0.8060344   1.0106121\n",
      "  -0.00867731]\n",
      " [-1.3528308   0.27248895  0.72915065 ... -1.0142437  -0.99884635\n",
      "   0.35030538]\n",
      " [-0.69977844  0.91831225 -1.6010174  ...  0.5959694  -0.25417027\n",
      "  -0.7087431 ]]\n",
      "tensor([[-0.6592,  0.9247, -1.5150,  ...,  0.5660, -0.2806, -0.7050],\n",
      "        [ 0.4682, -1.5894,  1.1343,  ...,  0.2199,  0.5147,  1.0605],\n",
      "        [ 0.0081,  2.1095, -1.4249,  ..., -1.0850, -0.3390, -0.5097],\n",
      "        ...,\n",
      "        [-0.5069, -0.5203, -0.5221,  ...,  0.7612,  0.9210, -0.0087],\n",
      "        [-1.3289,  0.2870,  0.7024,  ..., -0.9578, -0.9103,  0.3522],\n",
      "        [-0.6874,  0.9673, -1.5422,  ...,  0.5628, -0.2316, -0.7126]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Tensor-likes are not close!\n\nMismatched elements: 65467 / 65536 (99.9%)\nGreatest absolute difference: 4.343595027923584 at index (1, 16, 440) (up to 1e-05 allowed)\nGreatest relative difference: 1.0725865364074707 at index (0, 10, 440) (up to 1.3e-06 allowed)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(their_y[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# their_y = their_y.squeeze(0)\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtesting\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43massert_close\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheir_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mour_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/xtts/lib/python3.10/site-packages/torch/testing/_comparison.py:1530\u001b[0m, in \u001b[0;36massert_close\u001b[0;34m(actual, expected, allow_subclasses, rtol, atol, equal_nan, check_device, check_dtype, check_layout, check_stride, msg)\u001b[0m\n\u001b[1;32m   1508\u001b[0m error_metas \u001b[38;5;241m=\u001b[39m not_close_error_metas(\n\u001b[1;32m   1509\u001b[0m     actual,\n\u001b[1;32m   1510\u001b[0m     expected,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     msg\u001b[38;5;241m=\u001b[39mmsg,\n\u001b[1;32m   1526\u001b[0m )\n\u001b[1;32m   1528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_metas:\n\u001b[1;32m   1529\u001b[0m     \u001b[38;5;66;03m# TODO: compose all metas into one AssertionError\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_metas[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto_error(msg)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Tensor-likes are not close!\n\nMismatched elements: 65467 / 65536 (99.9%)\nGreatest absolute difference: 4.343595027923584 at index (1, 16, 440) (up to 1e-05 allowed)\nGreatest relative difference: 1.0725865364074707 at index (0, 10, 440) (up to 1.3e-06 allowed)"
     ]
    }
   ],
   "source": [
    "ours = PerceiverResampler(\n",
    "    dim=1024,\n",
    "    depth=2,\n",
    "    dim_context=1024,\n",
    "    num_latents=32,\n",
    "    dim_head=64,\n",
    "    heads=8,\n",
    "    ff_mult=4,\n",
    "    use_flash_attn=False,\n",
    "    key=jax.random.PRNGKey(1),\n",
    ")\n",
    "\n",
    "our_x = jax.random.normal(jax.random.PRNGKey(1), shape=(2, 32, 1024))\n",
    "their_x = torch.from_numpy(np.array(our_x))\n",
    "\n",
    "their_params = {key: value for key, value in model.named_parameters()}\n",
    "print(their_params.keys())\n",
    "\n",
    "print(model.latents.size())\n",
    "mapping = {\n",
    "    \"ff1.weight\": \"0.weight\",\n",
    "    \"ff1.bias\": \"0.bias\",\n",
    "    \"ff2.weight\": \"2.weight\",\n",
    "    \"ff2.bias\": \"2.bias\",\n",
    "}\n",
    "\n",
    "\n",
    "def update_weights(path, x):\n",
    "    seq = [str(p).strip(\"[].\") for p in path]\n",
    "    path = \".\".join(seq)\n",
    "    if path in their_params:\n",
    "        print(their_params[path].size())\n",
    "        # if \"bias\" in path:\n",
    "        #     return jnp.expand_dims(their_params[path].detach().numpy(), -1)\n",
    "        return jnp.array(their_params[path].detach().numpy())\n",
    "    if \"layers\" == seq[0] and \"1\" == seq[2]:\n",
    "        if \"ff1\" == seq[3]:\n",
    "            if \"bias\" in path:\n",
    "                return jnp.array(\n",
    "                    their_params[\".\".join([seq[0], seq[1], seq[2], \"0\", \"bias\"])]\n",
    "                    .detach()\n",
    "                    .numpy(),\n",
    "                )\n",
    "            else:\n",
    "                return jnp.array(\n",
    "                    their_params[\".\".join([seq[0], seq[1], seq[2], \"0\", \"weight\"])]\n",
    "                    .detach()\n",
    "                    .numpy()\n",
    "                )\n",
    "        if \"ff2\" == seq[3]:\n",
    "            if \"bias\" in path:\n",
    "                return jnp.array(\n",
    "                    their_params[\".\".join([seq[0], seq[1], seq[2], \"2\", \"bias\"])]\n",
    "                    .detach()\n",
    "                    .numpy(),\n",
    "                )\n",
    "            else:\n",
    "                return jnp.array(\n",
    "                    their_params[\".\".join([seq[0], seq[1], seq[2], \"2\", \"weight\"])]\n",
    "                    .detach()\n",
    "                    .numpy()\n",
    "                )\n",
    "    if path in mapping.keys():\n",
    "        if \"bias\" in path:\n",
    "            return jnp.array(their_params[mapping[path]].detach().numpy())\n",
    "        return jnp.array(their_params[mapping[path]].detach().numpy())\n",
    "    # print(path)\n",
    "    return x\n",
    "\n",
    "\n",
    "def print_shapes(path, x):\n",
    "    path = \".\".join([str(p).strip(\"[].\") for p in path])\n",
    "    if \"weight\" in path or \"bias\" in path:\n",
    "        print(f\"Path {path} and shape {x.shape}\")\n",
    "\n",
    "\n",
    "# print(f\"Latents: {ours.latents[0,0]}\")\n",
    "# jax.tree_util.tree_map_with_path(print_shapes, ours)\n",
    "ours = jax.tree_util.tree_map_with_path(update_weights, ours)\n",
    "# print(f\"Latents: {ours.latents[0,0]}\")\n",
    "# jax.tree_util.tree_map_with_path(print_shapes, ours)\n",
    "eqx.tree_serialise_leaves(\"xttsperciever.eqx\", ours)\n",
    "ours = eqx.tree_deserialise_leaves(\"xttsperciever.eqx\", ours)\n",
    "# print(f\"Latents: {ours.latents.shape}\")\n",
    "\n",
    "\n",
    "our_y = ours(our_x)\n",
    "their_y = model(their_x)\n",
    "\n",
    "print(our_y[0])\n",
    "print(their_y[0])\n",
    "# their_y = their_y.squeeze(0)\n",
    "\n",
    "torch.testing.assert_close(their_y, torch.from_numpy(np.array(our_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
